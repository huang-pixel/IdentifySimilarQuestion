{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/quora-question-pairs'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-18T16:38:02.447945Z","iopub.execute_input":"2023-02-18T16:38:02.448397Z","iopub.status.idle":"2023-02-18T16:38:02.458757Z","shell.execute_reply.started":"2023-02-18T16:38:02.448362Z","shell.execute_reply":"2023-02-18T16:38:02.457174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Structure ","metadata":{}},{"cell_type":"code","source":"# Unzip the input data file\nimport zipfile\n\nfilezip = filenames # get a list of all file in folder quora-question-pairs\n\nfor fz in filezip:\n    if fz.endswith(\".zip\"): # check the suffixe '.zip'\n        zFile = zipfile.ZipFile(dirname + \"/\" + fz,\"r\") # Zipfile Open a ZIP file\n        for fileM in zFile.namelist(): # .namelist() Return a list of archive members by name.\n            zFile.extract(fileM,\"/kaggle/working\")# .extract Extract a member from the archive to the current working directory","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:02.461492Z","iopub.execute_input":"2023-02-18T16:38:02.461923Z","iopub.status.idle":"2023-02-18T16:38:11.180678Z","shell.execute_reply.started":"2023-02-18T16:38:02.461889Z","shell.execute_reply":"2023-02-18T16:38:11.178431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the train set \ndf = pd.read_csv(\"/kaggle/working/train.csv\")\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:11.183641Z","iopub.execute_input":"2023-02-18T16:38:11.185000Z","iopub.status.idle":"2023-02-18T16:38:12.697635Z","shell.execute_reply.started":"2023-02-18T16:38:11.184919Z","shell.execute_reply":"2023-02-18T16:38:12.696386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the Quora Question consists of 404,290 raws and 6 columns.","metadata":{}},{"cell_type":"code","source":"# Summary statistics\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:12.699970Z","iopub.execute_input":"2023-02-18T16:38:12.700410Z","iopub.status.idle":"2023-02-18T16:38:12.780578Z","shell.execute_reply.started":"2023-02-18T16:38:12.700373Z","shell.execute_reply":"2023-02-18T16:38:12.779193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Information \ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:12.781886Z","iopub.execute_input":"2023-02-18T16:38:12.782548Z","iopub.status.idle":"2023-02-18T16:38:12.875346Z","shell.execute_reply.started":"2023-02-18T16:38:12.782511Z","shell.execute_reply":"2023-02-18T16:38:12.873289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:12.878493Z","iopub.execute_input":"2023-02-18T16:38:12.879130Z","iopub.status.idle":"2023-02-18T16:38:12.900695Z","shell.execute_reply.started":"2023-02-18T16:38:12.879070Z","shell.execute_reply":"2023-02-18T16:38:12.897510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspecting missing values \ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:12.902811Z","iopub.execute_input":"2023-02-18T16:38:12.903265Z","iopub.status.idle":"2023-02-18T16:38:12.985840Z","shell.execute_reply.started":"2023-02-18T16:38:12.903229Z","shell.execute_reply":"2023-02-18T16:38:12.984532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the missing values\ndf = df.dropna(how='any').reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:12.987848Z","iopub.execute_input":"2023-02-18T16:38:12.988218Z","iopub.status.idle":"2023-02-18T16:38:13.170858Z","shell.execute_reply.started":"2023-02-18T16:38:12.988186Z","shell.execute_reply":"2023-02-18T16:38:13.169362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the sample sentences :","metadata":{}},{"cell_type":"code","source":"# df.iloc[:10, [3,4]]\nfor i in range(0, 10):\n    print(df['question1'][i])\n    print(df['question2'][i])\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:13.175125Z","iopub.execute_input":"2023-02-18T16:38:13.175567Z","iopub.status.idle":"2023-02-18T16:38:13.184611Z","shell.execute_reply.started":"2023-02-18T16:38:13.175531Z","shell.execute_reply":"2023-02-18T16:38:13.183129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similarities Distributions ","metadata":{}},{"cell_type":"markdown","source":"Plot the distribution of the sentence duplicate or non duplicate : ","metadata":{}},{"cell_type":"code","source":"df['is_duplicate'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:13.185797Z","iopub.execute_input":"2023-02-18T16:38:13.186246Z","iopub.status.idle":"2023-02-18T16:38:13.207023Z","shell.execute_reply.started":"2023-02-18T16:38:13.186211Z","shell.execute_reply":"2023-02-18T16:38:13.205813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the proportion \n\ndf['is_duplicate'].value_counts() / len(df) * 100 ","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:13.208541Z","iopub.execute_input":"2023-02-18T16:38:13.210452Z","iopub.status.idle":"2023-02-18T16:38:13.226672Z","shell.execute_reply.started":"2023-02-18T16:38:13.210406Z","shell.execute_reply":"2023-02-18T16:38:13.225350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4,5))\nlabels = [\"unique\", \"duplicate\"]\nsizes = df['is_duplicate'].value_counts()\ncolors = ['palevioletred','dodgerblue']\n\nplt.title(\"Distribution of Similar Sentences\")\npatches,text1,text2 = plt.pie(sizes,\n                      explode=None,\n                      labels=labels,\n                      colors=colors,\n                      autopct = '%1.2f%%', \n                      shadow = False, \n                      startangle =90, \n                      pctdistance = 0.6) \n\nplt.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:13.228901Z","iopub.execute_input":"2023-02-18T16:38:13.229393Z","iopub.status.idle":"2023-02-18T16:38:13.363372Z","shell.execute_reply.started":"2023-02-18T16:38:13.229335Z","shell.execute_reply":"2023-02-18T16:38:13.361524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, let's draw a plot to visualize the correlation of word's length and the similarities : ","metadata":{}},{"cell_type":"code","source":"# The length of words\ndf['len_w_q1'] = df['question1'].apply(lambda x : len(str(x).split()))\ndf['len_w_q2'] = df['question2'].apply(lambda x : len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:13.366070Z","iopub.execute_input":"2023-02-18T16:38:13.367028Z","iopub.status.idle":"2023-02-18T16:38:14.525535Z","shell.execute_reply.started":"2023-02-18T16:38:13.366934Z","shell.execute_reply":"2023-02-18T16:38:14.524112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FuzzyWuzzy: Fuzzy String Matching in Python\nfrom fuzzywuzzy import fuzz\n\ndf['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:38:14.527183Z","iopub.execute_input":"2023-02-18T16:38:14.527572Z","iopub.status.idle":"2023-02-18T16:39:13.481365Z","shell.execute_reply.started":"2023-02-18T16:38:14.527537Z","shell.execute_reply":"2023-02-18T16:39:13.480131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find common words \ndf['common_words'] = df.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:13.482950Z","iopub.execute_input":"2023-02-18T16:39:13.483838Z","iopub.status.idle":"2023-02-18T16:39:22.134403Z","shell.execute_reply.started":"2023-02-18T16:39:13.483797Z","shell.execute_reply":"2023-02-18T16:39:22.133033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import plotly\n\nimport plotly.express as px\npx.histogram(df, x=\"len_w_q1\",height=700, color='is_duplicate', title=\"Question1 Length Distribution\", marginal=\"box\")","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:22.136055Z","iopub.execute_input":"2023-02-18T16:39:22.136538Z","iopub.status.idle":"2023-02-18T16:39:22.354358Z","shell.execute_reply.started":"2023-02-18T16:39:22.136491Z","shell.execute_reply":"2023-02-18T16:39:22.352540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(df, x=\"len_w_q2\",height=700, color='is_duplicate', title=\"Question2 Length Distribution\", marginal=\"box\")","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:22.356991Z","iopub.execute_input":"2023-02-18T16:39:22.357494Z","iopub.status.idle":"2023-02-18T16:39:22.570451Z","shell.execute_reply.started":"2023-02-18T16:39:22.357450Z","shell.execute_reply":"2023-02-18T16:39:22.568786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the boxplot and histogram, the data is skewed to the right as the tail is longer. The distribution of similar sentences is more in the sentences of relatively short in length.","metadata":{}},{"cell_type":"markdown","source":"## Word Cloud","metadata":{}},{"cell_type":"code","source":"# Import regex\nimport re \n\ndef text_cleaning(x):\n    \n    questions = re.sub('\\s+\\n+', ' ', x)\n    questions = re.sub('[^a-zA-Z0-9]', ' ', questions)\n    questions = questions.lower()\n    \n    return questions","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:22.572465Z","iopub.execute_input":"2023-02-18T16:39:22.573640Z","iopub.status.idle":"2023-02-18T16:39:22.580180Z","shell.execute_reply.started":"2023-02-18T16:39:22.573600Z","shell.execute_reply":"2023-02-18T16:39:22.578817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import tqdm to show the small progress \nfrom tqdm import tqdm\ntqdm.pandas()\n\ndf['q1_cleaned'] = df['question1'].progress_apply(text_cleaning)\ndf['q2_cleaned'] = df['question2'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:22.581812Z","iopub.execute_input":"2023-02-18T16:39:22.582171Z","iopub.status.idle":"2023-02-18T16:39:29.994573Z","shell.execute_reply.started":"2023-02-18T16:39:22.582139Z","shell.execute_reply":"2023-02-18T16:39:29.993269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['q1_cleaned', 'question1', 'q2_cleaned', 'question2']].head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:29.996279Z","iopub.execute_input":"2023-02-18T16:39:29.996640Z","iopub.status.idle":"2023-02-18T16:39:30.151128Z","shell.execute_reply.started":"2023-02-18T16:39:29.996607Z","shell.execute_reply":"2023-02-18T16:39:30.149826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\n\nquestion1 = df['q1_cleaned'].tolist()\nquestion2 = df['q2_cleaned'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:30.152897Z","iopub.execute_input":"2023-02-18T16:39:30.153443Z","iopub.status.idle":"2023-02-18T16:39:30.179652Z","shell.execute_reply.started":"2023-02-18T16:39:30.153396Z","shell.execute_reply":"2023-02-18T16:39:30.178244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words=1500,\n                      stopwords=STOPWORDS,\n                      random_state=42).generate(\" \".join(question1))\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Wordcloud Question1\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:30.181309Z","iopub.execute_input":"2023-02-18T16:39:30.182066Z","iopub.status.idle":"2023-02-18T16:39:49.334535Z","shell.execute_reply.started":"2023-02-18T16:39:30.182027Z","shell.execute_reply":"2023-02-18T16:39:49.331544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(background_color=\"white\",\n                      max_words=1500,\n                      stopwords=STOPWORDS,\n                      random_state=42).generate(\" \".join(question2))\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title(\"Wordcloud Question2\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:39:49.338000Z","iopub.execute_input":"2023-02-18T16:39:49.341009Z","iopub.status.idle":"2023-02-18T16:40:11.995293Z","shell.execute_reply.started":"2023-02-18T16:39:49.340960Z","shell.execute_reply":"2023-02-18T16:40:11.993786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word2Vec modeling ","metadata":{}},{"cell_type":"markdown","source":"Word2Vec is a more sophisticated word embedding technique. This technique is based on the idea that words that occur in the same contexts tend to have similar meanings.   \n\nSo if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words.","metadata":{}},{"cell_type":"markdown","source":"Simple graphic Word2Vec :\n\n**CBOW model** & **skip gram model**","metadata":{}},{"cell_type":"markdown","source":"![](https://blog.acolyer.org/wp-content/uploads/2016/04/word2vec-cbow.png)","metadata":{}},{"cell_type":"markdown","source":"![](https://blog.acolyer.org/wp-content/uploads/2016/04/word2vec-skip-gram.png)","metadata":{}},{"cell_type":"markdown","source":"Ref : [The amazing power of word vectors](http://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)","metadata":{}},{"cell_type":"markdown","source":"We will be using word2vec pre-trained **Google News corpus**. ","metadata":{}},{"cell_type":"code","source":"# Import KeyedVectors\n\nfrom gensim.models.keyedvectors import KeyedVectors\nmodel = KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:40:12.000786Z","iopub.execute_input":"2023-02-18T16:40:12.001204Z","iopub.status.idle":"2023-02-18T16:41:34.048893Z","shell.execute_reply.started":"2023-02-18T16:40:12.001170Z","shell.execute_reply":"2023-02-18T16:41:34.047806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Obtain sentence vector from word vector\n\nimport gensim\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\ndef sent2vec(s):\n    token_list = gensim.utils.simple_preprocess(str(s).encode('utf-8'))\n   \n    words = [w for w in token_list if w not in stop_words]\n    words = [w for w in words if w.isalpha()]\n    \n    words_vectors = []\n    for w in words:\n        try:\n            words_vectors.append(model[w])\n        except:\n            continue \n    # Summing the resulting word vectors and divide by the total number of vectors \n    # There are 300 vectors in Google's pre-trained model\n    words_vectors = np.array(words_vectors)\n    add_up_vectors = words_vectors.sum(axis=0)\n    sent_vectors = add_up_vectors / words_vectors.shape[0]\n    return sent_vectors","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:41:34.050694Z","iopub.execute_input":"2023-02-18T16:41:34.051426Z","iopub.status.idle":"2023-02-18T16:41:34.060784Z","shell.execute_reply.started":"2023-02-18T16:41:34.051381Z","shell.execute_reply":"2023-02-18T16:41:34.059823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.seterr(all='ignore', divide='ignore')\n\nquestion1_vectors = np.zeros((df.shape[0], 300))\n\nfor i, q in enumerate(tqdm(df['q1_cleaned'].values)):\n    question1_vectors[i,:] = sent2vec(q)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:41:34.061913Z","iopub.execute_input":"2023-02-18T16:41:34.062809Z","iopub.status.idle":"2023-02-18T16:42:11.467200Z","shell.execute_reply.started":"2023-02-18T16:41:34.062772Z","shell.execute_reply":"2023-02-18T16:42:11.465689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(question1_vectors.shape)\nprint(question1_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:42:11.469000Z","iopub.execute_input":"2023-02-18T16:42:11.469350Z","iopub.status.idle":"2023-02-18T16:42:11.476512Z","shell.execute_reply.started":"2023-02-18T16:42:11.469320Z","shell.execute_reply":"2023-02-18T16:42:11.475236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question2_vectors = np.zeros((df.shape[0], 300))\n\nfor i, q in enumerate(tqdm(df['q2_cleaned'].values)):\n    question2_vectors[i,:] = sent2vec(q)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:42:11.477835Z","iopub.execute_input":"2023-02-18T16:42:11.478203Z","iopub.status.idle":"2023-02-18T16:42:49.358694Z","shell.execute_reply.started":"2023-02-18T16:42:11.478172Z","shell.execute_reply":"2023-02-18T16:42:49.357627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(question2_vectors.shape)\nprint(question2_vectors)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:42:49.360584Z","iopub.execute_input":"2023-02-18T16:42:49.360974Z","iopub.status.idle":"2023-02-18T16:42:49.368104Z","shell.execute_reply.started":"2023-02-18T16:42:49.360940Z","shell.execute_reply":"2023-02-18T16:42:49.366831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now obtain the vector of sentences for question1 and question2, then we compute all the distances.","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cosine, jaccard, euclidean","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:42:49.369491Z","iopub.execute_input":"2023-02-18T16:42:49.369830Z","iopub.status.idle":"2023-02-18T16:42:49.381775Z","shell.execute_reply.started":"2023-02-18T16:42:49.369799Z","shell.execute_reply":"2023-02-18T16:42:49.380523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cosine_distance'] = [cosine(q1, q2) for (q1, q2) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['jaccard_distance'] = [jaccard(q1, q2) for (q1, q2) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['euclidean_distance'] = [euclidean(q1, q2) for (q1, q2) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n# Removing the Nan entries or infs entries","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:42:49.383648Z","iopub.execute_input":"2023-02-18T16:42:49.384477Z","iopub.status.idle":"2023-02-18T16:43:26.435649Z","shell.execute_reply.started":"2023-02-18T16:42:49.384427Z","shell.execute_reply":"2023-02-18T16:43:26.434473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.437589Z","iopub.execute_input":"2023-02-18T16:43:26.438890Z","iopub.status.idle":"2023-02-18T16:43:26.461556Z","shell.execute_reply.started":"2023-02-18T16:43:26.438846Z","shell.execute_reply":"2023-02-18T16:43:26.460217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.463168Z","iopub.execute_input":"2023-02-18T16:43:26.463721Z","iopub.status.idle":"2023-02-18T16:43:26.612351Z","shell.execute_reply.started":"2023-02-18T16:43:26.463679Z","shell.execute_reply":"2023-02-18T16:43:26.610980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Xgboost","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.614302Z","iopub.execute_input":"2023-02-18T16:43:26.614933Z","iopub.status.idle":"2023-02-18T16:43:26.620250Z","shell.execute_reply.started":"2023-02-18T16:43:26.614890Z","shell.execute_reply":"2023-02-18T16:43:26.619231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['question1', 'question2'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.621538Z","iopub.execute_input":"2023-02-18T16:43:26.622163Z","iopub.status.idle":"2023-02-18T16:43:26.740560Z","shell.execute_reply.started":"2023-02-18T16:43:26.622124Z","shell.execute_reply":"2023-02-18T16:43:26.739124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['q1_cleaned', 'q2_cleaned'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.742234Z","iopub.execute_input":"2023-02-18T16:43:26.742621Z","iopub.status.idle":"2023-02-18T16:43:26.773944Z","shell.execute_reply.started":"2023-02-18T16:43:26.742572Z","shell.execute_reply":"2023-02-18T16:43:26.772724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.775567Z","iopub.execute_input":"2023-02-18T16:43:26.775963Z","iopub.status.idle":"2023-02-18T16:43:26.791889Z","shell.execute_reply.started":"2023-02-18T16:43:26.775929Z","shell.execute_reply":"2023-02-18T16:43:26.790827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.loc[:, df.columns != 'is_duplicate']\ny = df.loc[:, df.columns == 'is_duplicate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.792992Z","iopub.execute_input":"2023-02-18T16:43:26.793366Z","iopub.status.idle":"2023-02-18T16:43:26.905384Z","shell.execute_reply.started":"2023-02-18T16:43:26.793331Z","shell.execute_reply":"2023-02-18T16:43:26.903842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.907464Z","iopub.execute_input":"2023-02-18T16:43:26.907890Z","iopub.status.idle":"2023-02-18T16:43:26.916622Z","shell.execute_reply.started":"2023-02-18T16:43:26.907840Z","shell.execute_reply":"2023-02-18T16:43:26.915116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.918515Z","iopub.execute_input":"2023-02-18T16:43:26.919058Z","iopub.status.idle":"2023-02-18T16:43:26.931531Z","shell.execute_reply.started":"2023-02-18T16:43:26.919014Z","shell.execute_reply":"2023-02-18T16:43:26.930119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\nmodel = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, subsample=0.8).fit(X_train, y_train.values.ravel()) ","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:43:26.933370Z","iopub.execute_input":"2023-02-18T16:43:26.933792Z","iopub.status.idle":"2023-02-18T16:45:01.906390Z","shell.execute_reply.started":"2023-02-18T16:43:26.933727Z","shell.execute_reply":"2023-02-18T16:45:01.904994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(X_test)\ncm = confusion_matrix(y_test, prediction)  \nprint(cm)  \nprint('Accuracy', accuracy_score(y_test, prediction))\nprint(classification_report(y_test, prediction))","metadata":{"execution":{"iopub.status.busy":"2023-02-18T16:45:46.317182Z","iopub.execute_input":"2023-02-18T16:45:46.317688Z","iopub.status.idle":"2023-02-18T16:45:48.440421Z","shell.execute_reply.started":"2023-02-18T16:45:46.317647Z","shell.execute_reply":"2023-02-18T16:45:48.438727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}